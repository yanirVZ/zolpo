import os
import json
import requests
from urllib.parse import urlparse
import urllib3
import gzip

# ×‘×™×˜×•×œ ××–×”×¨×•×ª SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

COOKIES_FILE = "cookies.json"
LINKS_FILE = "valid_links.txt"
DOWNLOAD_FOLDER = r"C:\zolpo\rami_levi_prices"
STORE_MAP = {}

def load_cookies():
    with open(COOKIES_FILE, "r") as f:
        raw = json.load(f)
    cookies = {}
    for cookie in raw:
        cookies[cookie["name"]] = cookie["value"]
    return cookies

def load_store_map():
    """ ×˜×•×¢×Ÿ ××ª ×”××™×¤×•×™ ×©×œ ×”×¡× ×™×¤×™× ×Ö¾Stores.json """
    store_file = "Stores7290058140886-001-202504101000.json.gz"  # ×™×© ×œ×”×•×¨×™×“ ××ª ×§×•×‘×¥ ×”Ö¾Stores
    if os.path.exists(store_file):
        with gzip.open(store_file, 'rt', encoding='utf-8-sig') as f:
            store_data = json.load(f)
            for store in store_data.get("Stores", []):
                store_id = store["StoreId"]
                store_name = store["StoreName"]
                STORE_MAP[store_id] = store_name
        print("ğŸª ××™×¤×•×™ ×¡× ×™×¤×™× × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”.")
    else:
        print("âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ Stores, ×”×•×¨×“ ××•×ª×• ×§×•×“×.")

def download_all():
    os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)
    cookies = load_cookies()
    session = requests.Session()
    session.cookies.update(cookies)

    # ×§×¨×™××” ×œ×§×•×‘×¥ ×”Ö¾Stores ×× ×”×•× ×œ× ×˜×•×¢×Ÿ ×›×‘×¨
    load_store_map()

    with open(LINKS_FILE, "r") as f:
        links = [line.strip() for line in f if line.strip()]

    count = 0
    for url in links:
        filename = os.path.basename(urlparse(url).path)
        dest = os.path.join(DOWNLOAD_FOLDER, filename)

        # ×× ×–×” ×§×™×©×•×¨ ×œ- Stores, × ×•×¨×™×“ ××•×ª×•
        if "Stores" in url:
            try:
                print(f"ğŸ“¥ ××•×¨×™×“ ×§×•×‘×¥ Store: {url}")
                resp = session.get(url, verify=False, stream=True)
                with open(dest, "wb") as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        f.write(chunk)
                print(f"âœ… ×”×•×¨×“: {filename}")
                load_store_map()  # × ×˜×¢×™×Ÿ ××ª ××™×¤×•×™ ×”×¡× ×™×¤×™× ××—×¨×™ ×”×•×¨×“×ª ×”-Store
                count += 1
            except Exception as e:
                print(f"âš ï¸ ×©×’×™××” ×‘×”×•×¨×“×ª {filename}: {e}")
        elif "Price" in url:
            try:
                print(f"ğŸ“¥ ××•×¨×™×“ ×§×•×‘×¥ ××—×™×¨: {url}")
                resp = session.get(url, verify=False, stream=True)
                if resp.ok and resp.headers.get("Content-Type", "").startswith("application"):
                    with open(dest, "wb") as f:
                        for chunk in resp.iter_content(chunk_size=8192):
                            f.write(chunk)
                    print(f"âœ… ×”×•×¨×“: {filename}")
                    count += 1
                else:
                    print(f"âŒ × ×›×©×œ: {filename} (HTML ×‘××§×•× GZIP?)")
            except Exception as e:
                print(f"âš ï¸ ×©×’×™××” ×‘×”×•×¨×“×ª {filename}: {e}")

    print(f"\nğŸ“¦ ×¡×™×™×× ×•. ×”×•×¨×“×• {count} ×§×‘×¦×™×.")

if __name__ == "__main__":
    download_all()
